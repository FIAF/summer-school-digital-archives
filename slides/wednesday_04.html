<!DOCTYPE html>
<html>
  <head>
    <title>Digital Summer School 2024: WED03</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="../style.css">  </head>
  <body>
    <textarea id="source">




class: center, middle, titlepage
### WED03: *Extracting content metadata.*




---
class: contentpage
### **Content Metadata**

We have looked a bit at extracting technical metadata from media, but now we are going to look at something else - extracting metadata around the content of the media.

This is an area which is being revolutionised by Large Language Models aka Machine Learning aka AI (like ChatGPT).


---
class: contentpage
### **Content Metadata**


Prior to around 2022, there were many projects in this space, using a variety of different techniques, often stretching back decades.

Speech to text.    
OCR.    
Image analysis.    
Experimental/Other.

---
class: contentpage
### **Content Metadata**

Prior to around 2022, there were many projects in this space, using a variety of different techniques, often stretching back decades.

~~Speech to text.~~ -> Whisper    
~~OCR.~~ -> LLM        
~~Image analysis.~~ -> LLM     
~~Experimental/Other.~~ -> LLM   

Advances in Machine Learning models are really taking over, but this is very much a constantly evolving area with new models being released often.



---
class: contentpage
### **Machine Learning**


These systems all have in common the idea of an artificial neural network, an idea which has been around since the 1940s, but has only achieved recent widespread use due to computational power and innovative strategies.

An artificial neural network very roughly approximates the way our own brains work, in that they comprise many many many very simple "nodes", which develop to behave in certain ways, and complexity arises out of many many nodes communicating with many many other nodes.

---
class: contentpage
### **Machine Learning**

A traditional artificial neural network has three components, "inputs", "hidden layers", and "outputs".

- The inputs could be numbers, letters, pixels, audio samples, as long as the expressed as numbers.

- The hidden layers sit in-between. These are sequential layers of nodes which are all interconnected to one direction. Often these would start off as randomised noise, so will produce useless results, but if we have known input which result in expected outputs, we can start to "train" all the nodes in the hidden layer to start to behave in a certain way.

- The outputs could be as simple as a "yes" or "no" answer to the question, "is this a Spanish word?"

The power of artificial neural networks are that they can apply to almost any subject or discipline, the downside is that the hidden layers are not explicable - we get to a place where we cannot explain how a certain trained neural net (or "model") is producing a certain result.

Depending on source material, we can also see undesired biases replicated. 

[NNFS](https://nnfs.io/) is a wonderful resource which contains examples of building a neural network using just vanilla Python.

Of course, like cars, we don't have to understand fully how artifical neural networks work to use them!!

---
class: contentpage
### **Whisper**


With this context, we can now start to look at Whisper from OpenAI. 

Whisper is now so ubiquitous it has almost completely replaced all other competitors and all other speech to text systems.

It returns speech-to-text "cooked", with punctuation and disfluences (this is a new word for me) removed. As with all neural network models, it comes in a variety of "sizes" dependent on complexity, which generally correlate to quality of results.


---
class: contentpage
### **Whisper**

A minimum viable command for whisper is 

```sh
whisper media/film_scan/audio.wav
```

which will use default values for the "model" (neural network discussed before) the language and the output format.




---
class: contentpage
### **Whisper**


The first flag we might want to feed whisper is the language, otherwise it will take a sample of the first 30 seconds and automatically detect.

As an aside, I have noticed that anytime Whisper cannot detect a language if seems to think it is "Welsh".

```sh
whisper --language en media/film_scan/audio.wav
```



---
class: contentpage
### **Whisper**

output_dir lets us specify an output directory 

```sh
whisper --language en --output_dir whisper_test/ media/film_scan/audio.wav
```



---
class: contentpage
### **Whisper**


while we are at it, we may also want to pick an output format.

SRTs and VTTs are subtitle tracks with timestamps, which you should be able to drop straight into VLC or DCPs you are creating.

JSON and TSV are useful as structured data, using formats which are common for storing machine-readable data.

We are for now going to use the simplest - a TXT file.

```sh
whisper --language en --output_dir whisper_test/ --output_format txt media/film_scan/audio.wav
```





---
class: contentpage
### **Whisper**

As mentioned before we can also set the size of the model which is used: base, small, medium, large.

It is worth doing some tests to see what effect the bigger the model has, it will use more disk space and take longer to process, so worth checking the results to see if they are significant enough to justify this.


Matt Miller at Library of Congress has done something similar.
https://thisismattmiller.com/post/lomax-whisper/


```sh
whisper --language en --output_dir whisper_test/ --output_format txt --model large-v2 media/film_scan/audio.wav
```

PRACTICAL: compare results of base and large-v2 side by side.

Also, there is nothing restricting you to only using the models provided by OpenAI.



---
class: contentpage
### **LLMs**

Now to shift focus to other tools using similar technology, LLMs or Large Language Models.

The most famous of these is undoubtedly ChatGPT from OpenAI, but there are now equivalent models from Meta (Llama), Microsoft (Phi) and Google (Gemma).

We might begin by looking at different things we can do with these models, and then different ways to access them.



---
class: contentpage
### **LLMs**

We are going to dive right in with a multi-modal model called LLaVA, which allows us to do image analysis. Note that we are now about to discover how powerful (or not) our computers are.

Ollama is a cross-platform command line application which us lets run machine learning models very easily from our own machines.

https://ollama.com/

There is a wide selection of models on offer, and as with whisper they range in size, with the expectation that bigger the model better the results (apparently not always true)!

https://ollama.com/library/gemma2

---
class: contentpage
### **LLMs**

Once we have Ollama installed, we can easily call our required model using the following command.

```sh
ollama run llava:7b
```

Bigger models may have systems requirements beyond what our computer can handle, in which case we will get an error message to inform us of this.

---
class: contentpage
### **LLMs**



Back in time if you wanted to do OCR, object detection, or other visual analysis, this would have required dedicated processes. LLMs are a bit different, they are capable of doing it all, we just need to ask the right questions.

---
class: contentpage
### **LLMs**



General description example

```sh
Can you describe image '/home/paulduchesne/Desktop/summer-school-2024/jpg/0087679.jpg'
```



---
class: contentpage
### **LLMs**

*OCR*


```sh
What text is in this image '/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'
```

Please note that accuracy can depend on model size, so this is the one section of the course where having a powerful computer will give you better results, not just faster.
I am currently running the lowest model as my laptop is nothing special, would be keen to compare with people running something better.

For anyone who remembers using Tesseract, mind-bendingly we can ask for specific interpreted text.

```sh
What is the name of the boat in this image '/home/paulduchesne/Desktop/summer-school-2024/jpg/0087024.jpg'
```

A problem I noticed during testing this was that LLaVA can sometimes be confused when new images are supplied consecutivelu in the same session, so quiting in between may not be a terrible idea.



---
class: contentpage
### **LLMs**

*Object Detection*


Frame with boat.

```sh
What objects are visible in '/home/paulduchesne/Desktop/summer-school-2024/jpg/0087024.jpg'
```


Frame with flags.

```sh
What objects are visible in '/home/paulduchesne/Desktop/summer-school-2024/jpg/0087200.jpg'
```




---
class: contentpage
### **LLMs**


*Image Characteristics*


```sh
Is the following image black and white or colour: '/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'
```



```sh
What is the aspect ratio of the image area of the following film frame (eg 1.66, 1.85): '/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'
```

This is especially interesting as the image is letterboxed. There are much easier ways to do "letterbox detection" (e.g. with FFmpeg), but those can be susceptible to black levels.




```sh
What period do you think the following image is from: '/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'
```


---
class: contentpage
### **LLMs**

So far we have been returning text, which is great, but as we want to integrate with databases and automated systems we want a machine-readable output - LLMs can do this as well.

```sh
Return whether the following image is black and white or colour as a JSON result, following the example {"colour":"Black and White"} or {"colour":"Colour"} and do not return any other text than the JSON: '/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'
```

---
class: contentpage
### **LLMs**


Now the only piece missing is automating this - copying and pasting lines of text into the terminal is not going to scale if we want to inspect hundreds, thousands or millions of images.

Basic Python example.

```python
import ollama

request = ollama.chat(
    model='llava:7b',
    messages=[
        {
        'role': 'user',
        'content': 'What is the aspect ratio of the image area of the following film frame (eg 1.66, 1.85)',
        'images': ['/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg'],
        }
    ]
)

print(request['message']['content'])
```



---
class: contentpage
### **LLMs**

Convert to a reusable function.

```python
import ollama

def aspect_ratio_detector(frame):
    
    request = ollama.chat(
        model='llava:7b',
        messages=[
            {
            'role': 'user',
            'content': 'What is the aspect ratio of the image area of the following film frame (eg 1.66, 1.85)',
            'images': [frame],
            }
        ]
    )

    return request['message']['content']

result = aspect_ratio_detector('/home/paulduchesne/Desktop/summer-school-2024/jpg/0086556.jpg')

print(result)
```





---
class: contentpage
### **LLMs**

Iterate over the full directory of scans.

On my computer this would take a very long time, and there would be little value in processing every image - you would probably want to reduce to one frame per second, or less.

```python

import ollama

jpg_path = pathlib.Path.cwd() / 'jpg'
for frame in jpg_path.iterdir():
    result = cool_llm_process(frame)
    print(frame, result)
```








class: center, middle, titlepage
### WED04: *External Connections.*

---
class: contentpage
### **Agenda**


In this session, we are going to talk about "external connections". 

Up until now, all our processes have been happening locally, but what if we seek to connect the results up with the outside world?

This could be thought of as knowledge enrichment, benefiting from knowledge available elsewhere, but also as a step to ensuring explicit shared definitions - when I say "DPX", does it mean the same thing as when you say "DPX"?

You will notice that Wikidata will feature very heavily in this section, as it is the largest structured open knowledge resource on the web. 


---
class: contentpage
### **Agenda**


1. DROID        
    1.1 PRONOM     
2. Text Wikification
3. Collection Wikification     
3.1 Linking    
3.2 Leveraging    



---
class: contentpage
### **1. DROID**

DROID is a very interesting tool from the UK National Archives which profiles files, meaning determining the type of the file (DPX version 2.0), beyond just the extension (.dpx).

It is also very useful if you are given a large collection of diverse digital files, and you want to automatically figure out what you have been given.

https://digital-preservation.github.io/droid/





---
class: contentpage
### **1. DROID**



To run DROID from the command line we are going to want to download the "bin" from the DROID GitHub releases page.

https://github.com/digital-preservation/droid/releases

Once we have unpacked the bin.zip we can run the following to show everything is working.

```sh
java -jar droid-command-line-6.8.0.jar -v
```

And we can now use it on a directory, here using our scans. This is not especially interesting as the film scans are quite homogenous, but try pointing towards this at your Downloads directory and seeing what reports you get.

```sh
java -jar droid-command-line-6.8.0.jar -a /home/paulduchesne/Desktop/summer-school-2024/media -o /home/paulduchesne/Desktop/droid_test.csv
```


---
class: contentpage
### **1. DROID**


Just to unpack what is happening here, DROID is processing each file and checking that the "signatures" of the file (which we saw earlier) are correct and match the extension. This is to catch any instances where someone has taken an "image.jpg" and renamed as "image.dpx", a feature we can test ourselves.


What is especially interesting though, and the focus of this session, is the PUID column, which is printing the PRONOM identifier.

---
class: contentpage
### **1.1 PRONOM**


These PUIDs connect to the PRONOM technical registry, also managed by the UK National Archives.

https://www.nationalarchives.gov.uk/PRONOM/Default.aspx

This becomes useful, because it turns out there are different types of file which use the "dpx" extension.

By moving from referring to a file as a "dpx file" to "fmt/193" we establish exactly what type of file we are talking about to prevent confusion.



---
class: contentpage
### **1.1 PRONOM**

PRONOM also provides great context around the file, where is it from, what are some technical qualities.

https://www.nationalarchives.gov.uk/PRONOM/Format/proFormatSearch.aspx?status=detailReport&id=669



---
class: contentpage
### **1.1 PRONOM**



It also links to Wikidata, which is the machine-readable sister project to Wikipedia (which we will look at in more depth shortly).

https://www.wikidata.org/wiki/Q1676669

Wikidata links to the "preservation plan" of the NARA (National Archives and Records Administration in the US)

https://www.archives.gov/files/lod/dpframework/id/NF00492.ttl

Which in turn gives us a "risk level", "appropriate tools" and a "preservation plan status".


---
class: contentpage
### **1.1 PRONOM**

For me this is exciting stuff, it is still early days for a lot of these systems, but the vision is that we are moving towards a space where we can ask questions like "how much of my digital collection is at risk" using shared risk analysis.

These are import questions for a digital archive.


We could also write a script which runs DROID, extracts the PUIDs, consults Wikidata, and returns NARA risk levels as a single process. 
Remind me if we end up with spare time to try this!

---
class: contentpage
### **1.1 PRONOM**

Also, if this subject is specifically interesting to you, I would recommend having a read of the DPC guide to "Wikidata for Digital Preservationists"

https://www.dpconline.org/docs/technology-watch-reports/2551-thorntonwikidatadpc-revsionthornton/file








---
class: contentpage
### **2. Text Wikification**


The first section covered connecting technical attributes externally, but now we can also revisit the content extraction outputs from the last session and look at a process called Wikification.


This plays into a broader field called Named Entity Recognition, which essentially boils down to extracting "entities" from text. There is an online example of this concept here: https://demos.explosion.ai/displacy-ent
"Paul grew up in Australia" becomes

.left[<img src="../img/ner.png" width="300">]

---
class: contentpage
### **2. Text Wikification**




This gets us so far, in that we now know that Paul is a person - but which person? This is important in making your collection searchable, because researchers and users are likely to be looking for a specific "Paul".

We need an ID (person/76276), or a URI (https://viaf.org/viaf/51677529/) to apply to the Named Entity, and given the availability and ubiquity of Wikipedia, it can be used to fulfil this function.

---
class: contentpage
### **2. Text Wikification**


Machine-learning are also a game changer in this space, while there were absolutely techniques for doing this stretching back decades, ML performs better and can jump through multiple processes at once.

An example prompt to feed to an LLM:

```txt
Take the following text and determine all spaCY entities and types, 
then add valid wikipedia links, using Markdown link syntax. For 
example if the input was "Annie Ernaux wrote the book The Years", 
return "[Annie Ernaux](https://en.wikipedia.org/wiki/Annie_Ernaux) 
wrote the book [The Years](https://en.wikipedia.org/wiki/The_Years_(Ernaux_book))". 
Do not return a wikipedia link if the wikipedia page does not really exist. 
Here is the source text: "David Lean directed the film Lawrence of Arabia.",
```

Note we are switching from a LLaVA model to Llama3.1, as we do not need all of the media processing functions, this is text only.




---
class: contentpage
### **2. Text Wikification**

This is still an emerging field for archives, with the promise of revolutionising the granularity of search. 

A few considerations to be aware of:

- Whether the Wikipedia link actually exists. Tests on low-size models showed a willingness to invent "likely" Wikipedia addresses which just did not exist (e.g. "https://en.wikipedia.org/wiki/Lawrence_of_Arabia_(David_Lean_film)").
- Whether the Wikipedia link is being properly disambiguated based on context, e.g. "It is a short drive from Sydney to Newcastle" returning https://en.wikipedia.org/wiki/Newcastle,_New_South_Wales, not https://en.wikipedia.org/wiki/Newcastle_upon_Tyne or https://en.wikipedia.org/wiki/Newcastle_(film)



---
class: contentpage
### **2. Text Wikification**



We can write a simple function to process the Whisper text extraction of our film scan and see what we get out!


```python
import ollama
import pathlib

def text_wikification(source_text):
    request = ollama.chat(
        model='llama3.1:8b',
        messages=[
            {
            'role': 'user',
            'content':  'Take the following text and determine all spaCY entities and types, then add valid wikipedia links, \
                using Markdown link syntax. For example if the input was "Annie Ernaux wrote the book The Years", return \
                "[Annie Ernaux](https://en.wikipedia.org/wiki/Annie_Ernaux) wrote the book [The Years](https://en.wikipedia.org/wiki/The_Years_(Ernaux_book))". \
                Do not return a wikipedia link if the wikipedia page does not really exist. Return the annotated text only, no other information. \
                Here is the source text: "'+source_text
            }
        ]
    )

    return request['message']['content']
```

---
class: contentpage
### **2. Text Wikification**




```python
with open(pathlib.Path.cwd() / 'whisper_test'/ 'audio.txt') as text_in:
    text_in = text_in.read()
    processed_text = text_wikification(text_in)

print('SOURCE_TEXT', '\n', text_in, '\n')
print('PROCESSED_TEXT', '\n', processed_text, '\n')

```


Quality of results will vary depending on the model and prompt used. For example, I have seen very very good results from the Gemini Pro Google models, which are not currently available through Ollama.

A more time-consuming possible method would be to use a system like GLiNER (https://github.com/urchade/GLiNER) to first identify all entities, and then use a system like Llama to perform context aware Wikipedia linking one at a time?




---
class: tangentpage
### **Tangent: What is Markdown?**

In the previous examples we were producing text formatted as "Markdown", thought it worth cycling back and explaining markdown to anyone who this may be a new concept.

Markdown is simply a method of annotating a text file so that it can contain formatting (bold, italics, links, tables and images) without requiring a proprietary and closed application to render (eg Word).

This is good as it means that your documents remain application agnostic, and you don't have to be concerned about access in the future.

Here we had used the markdown "link" syntax to attach a Wikipedia link to an "entity". This is not exactly what it was designed for, but works perfectly fine for our purposes.

---
class: tangentpage
### **Tangent: What is Markdown?**


These slides are all written as Markdown! 

https://github.com/paulduchesne/summer-school-2024/blob/main/slides/wednesday_04.html

If you want more information on how to use Markdown, this is a good guide: 

https://www.markdownguide.org/





---
class: center, middle, titlepage
### End of Day Three.





      </textarea>
      <script src="https://remarkjs.com/downloads/remark-latest.min.js" type="text/javascript"></script>
      <script type="text/javascript">var slideshow = remark.create({ratio: "16:9"});</script>
    </body>
  </html>